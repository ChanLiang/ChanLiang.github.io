---
title: "Research Portfolio"
excerpt: "An overview of LLM trustworthiness, covering robustness, transparency, validity, and resistance to misuse."
collection: portfolio
---


<!-- ![LLM Trustworthiness](/images/llm-trustworthiness.png) -->

<!-- <img src="/images/llm-trustworthiness.png" alt="LLM Trustworthiness" style="width: 65%; display: block; margin: auto;" /> -->
<img src="/images/trust-llm.png" alt="LLM Trustworthiness" style="width: 65%; display: block; margin: auto;" />

The diagram illustrates the four key aspects of LLM trustworthiness:
- **Robustness to Input**: Ensuring LLMs can handle adversarial attacks and distribution shifts ([ICLR 2025](https://openreview.net/pdf?id=txoJvjfI9w)).
- **Transparency of Decision**: Improving reasoning models and long-chain-of-thought ([Coming Soon](#)).
- **Validity of Output**: Addressing factual errors ([EMNLP 2023](https://aclanthology.org/2023.emnlp-main.390)) and inconsistencies in outputs ([ACL 2023 findings](https://aclanthology.org/2023.findings-acl.462)).
- **Resistance to Misuse**: Preventing cheating, plagiarism ([ACL 2024](https://aclanthology.org/2024.acl-long.496.pdf)), and unsafe fine-tuning ([ICML 2025](https://icml.cc/virtual/2025/poster/45951)).

<!-- <img src="/images/llm-trustworthiness-sys.png" width="65%" />
 -->
<img src="/images/trust-llm-sys.png" style="width: 65%; display: block; margin: auto;" />


From a systems perspective, the first three aspects correspond to the input, hidden states, and output of the system. The final aspect represents the relationship between the system and its users. These principles guide the design of safer and more trustworthy large language models.

_Last updated: June 2025_
